# -*- coding: utf-8 -*-
"""lightGB_randomizedsearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NU86D2Fvl8TrB7STwLDc4qqi6Hy4dq0C
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

input_data = pd.read_excel('Input_X.xlsx')
output_data = pd.read_excel('Output_Y.xlsx')

input_data.head()

output_data.head()

# Checking for missing values in input and output data
print("Missing values in input data:")
print(input_data.isnull().sum())

print("\nMissing values in output data:")
print(output_data.isnull().sum())

# Replace zeros with NaN to treat them as missing values
input_data.replace(0, np.nan, inplace=True)

# Drop rows with any NaN values (this will drop rows with original NaNs or replaced zeros)
input_data.dropna(inplace=True)

# Reset the index after dropping rows
input_data.reset_index(drop=True, inplace=True)

X = input_data
y = output_data['CHL_a']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# !pip install dask[dataframe]

# Import necessary libraries
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Initialize the LightGBM model
lgbm = lgb.LGBMRegressor(random_state=42)

# Define the parameter distribution for RandomizedSearchCV
param_dist_lgbm = {
    'num_leaves': [31, 50, 70, 100, 200],
    'max_depth': [-1, 10, 20, 30, 50],
    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'n_estimators': [100, 200, 500, 1000, 1500],
    'min_child_samples': [20, 50, 100, 150],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],
    'reg_alpha': [0, 0.1, 0.5, 1, 5],
    'reg_lambda': [0, 0.1, 0.5, 1, 5]
}

# Initialize RandomizedSearchCV for LightGBM
random_search_lgbm = RandomizedSearchCV(
    estimator=lgbm,
    param_distributions=param_dist_lgbm,
    n_iter=50,  # Number of random combinations to try
    cv=3,  # 3-fold cross-validation
    n_jobs=-1,  # Use all available cores
    scoring='neg_mean_squared_error',
    verbose=2,
    random_state=42
)

# Train the model with RandomizedSearchCV
random_search_lgbm.fit(X_train, y_train)

# Get the best model
best_lgbm = random_search_lgbm.best_estimator_

# Print the best parameters found
print("Best parameters for LightGBM:", random_search_lgbm.best_params_)

# Evaluate the model on training data
y_train_pred = best_lgbm.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_train_pred)
print(f"Training - MSE: {mse_train:.4f}, RMSE: {rmse_train:.4f}, R2: {r2_train:.4f}")

# Evaluate the model on testing data
y_test_pred = best_lgbm.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"Testing - MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}, R2: {r2_test:.4f}")

import matplotlib.pyplot as plt

# Plot for training data
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5, color='blue', edgecolor='k')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')
plt.xlabel("True CHL_a Values (Training)")
plt.ylabel("Predicted CHL_a Values (Training)")
plt.title("LightGBM: True vs Predicted (Training)")
plt.xscale('log')
plt.yscale('log')
plt.xlim(10**-2, y_train.max())
plt.ylim(10**-2, y_train.max())
plt.gca().set_aspect('equal', adjustable='box')

# Plot for testing data
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5, color='green', edgecolor='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("True CHL_a Values (Testing)")
plt.ylabel("Predicted CHL_a Values (Testing)")
plt.title("LightGBM: True vs Predicted (Testing)")
plt.xscale('log')
plt.yscale('log')
plt.xlim(10**-2, y_test.max())
plt.ylim(10**-2, y_test.max())
plt.gca().set_aspect('equal', adjustable='box')

plt.tight_layout()
plt.show()

