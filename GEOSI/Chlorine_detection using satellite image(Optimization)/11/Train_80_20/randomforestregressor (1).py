# -*- coding: utf-8 -*-
"""randomforestregressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jn7B3Ki6FS4gFz3IXU6kuh3xSUbzqXOh
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

input_data = pd.read_excel('Input_X.xlsx')
output_data = pd.read_excel('Output_Y.xlsx')

input_data.head()

output_data.head()

# Checking for missing values in input and output data
print("Missing values in input data:")
print(input_data.isnull().sum())

print("\nMissing values in output data:")
print(output_data.isnull().sum())

# Replace zeros with NaN to treat them as missing values
input_data.replace(0, np.nan, inplace=True)

# Drop rows with any NaN values (this will drop rows with original NaNs or replaced zeros)
input_data.dropna(inplace=True)

# Reset the index after dropping rows
input_data.reset_index(drop=True, inplace=True)

X = input_data
y = output_data['CHL_a']

y = y.values.ravel()  # Flatten to 1D array, as required by the model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

param_dist_rf = {
    'n_estimators': [400, 500, 600],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Initialize and fit Randomized Search with Random Forest
rf_model = RandomForestRegressor(random_state=42)
random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_dist_rf, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)
random_search_rf.fit(X_train, y_train)

# Best Random Forest model
best_rf = random_search_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test)

# Evaluation and plot
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest MSE: {mse_rf}, RMSE: {rmse_rf}, R2: {r2_rf}")

import matplotlib.pyplot as plt

plt.figure(figsize=(7, 7))
plt.scatter(y_test, y_pred_rf, alpha=0.5, color="blue")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xscale('log')
plt.yscale('log')
plt.xlim(10**-2, y_test.max())
plt.ylim(10**-2, y_test.max())
plt.xlabel("True CHL_a Values")
plt.ylabel("Predicted CHL_a Values")
plt.title("Random Forest: True vs Predicted")
plt.gca().set_aspect('equal', adjustable='box')
plt.show()

# Train your model
random_search_rf.fit(X_train, y_train)

import joblib

# Save the trained Random Forest model to a file
joblib.dump(random_search_rf, 'random_forest_model.pkl')
# Load the trained Random Forest model from a file
rf_loaded = joblib.load('random_forest_model.pkl')

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import joblib

# Step 1: Load the datasets
X = pd.read_excel('Input_X.xlsx')
y = pd.read_excel('Output_Y.xlsx')

# Step 2: Ensure the target variable 'y' is aligned as a single column
if y.shape[1] > 1:
    y = y.iloc[:, 0]  # Adjust to use the correct column as your target if necessary

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Set up the Randomized Search parameters for Random Forest
param_distributions = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf = RandomForestRegressor(random_state=42)
rf_random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_distributions,
    n_iter=50,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Step 5: Fit the model using Randomized Search
rf_random_search.fit(X_train, y_train)

# Step 6: Print the best parameters and save the model
print("Best parameters found:", rf_random_search.best_params_)
best_rf_model = rf_random_search.best_estimator_
joblib.dump(best_rf_model, 'random_forest_model.pkl')

# Step 7: Evaluate the model on training data
y_train_pred = best_rf_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_train_pred)
print(f"Training - MSE: {mse_train}, RMSE: {rmse_train}, R2: {r2_train}")

# Step 8: Evaluate the model on testing data
y_test_pred = best_rf_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"Testing - MSE: {mse_test}, RMSE: {rmse_test}, R2: {r2_test}")

print(y.shape)  # Check the shape of the target variable

print(X.shape)

import matplotlib.pyplot as plt

# Assuming y_test and y_test_pred are already defined as actual vs predicted values

plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_test_pred, alpha=0.6, color="blue")  # Scatter plot of true vs predicted
plt.plot([y_test.min().item(), y_test.max().item()], [y_test.min().item(), y_test.max().item()], 'r--')  # Ideal line (y=x)
plt.xscale('log')  # Log scale for x-axis
plt.yscale('log')  # Log scale for y-axis
# Accessing the values with .item() ensures we get numerical values
plt.xlim(10**-2, y_test.max().item())  # Set x-axis limits
plt.ylim(10**-2, y_test.max().item())  # Set y-axis limits
plt.xlabel("True CHL_a Values")  # Label for x-axis
plt.ylabel("Predicted CHL_a Values")  # Label for y-axis
plt.title("Random Forest: True vs Predicted")  # Title of the plot
plt.gca().set_aspect('equal', adjustable='box')  # Equal scaling of axes
plt.show()

